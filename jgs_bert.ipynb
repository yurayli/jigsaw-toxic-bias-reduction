{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py:243: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\r\n",
      "  cmdoptions.check_install_build_global(options)\r\n",
      "Created temporary directory: /tmp/pip-ephem-wheel-cache-kmf8rmp8\r\n",
      "Created temporary directory: /tmp/pip-req-tracker-bmyydhxe\r\n",
      "Created requirements tracker '/tmp/pip-req-tracker-bmyydhxe'\r\n",
      "Created temporary directory: /tmp/pip-install-axhnxcy0\r\n",
      "Processing /kaggle/input/nvidia-apex/apex-880ab92\r\n",
      "  Created temporary directory: /tmp/pip-req-build-o15qdqbv\r\n",
      "  Added file:///kaggle/input/nvidia-apex/apex-880ab92 to build tracker '/tmp/pip-req-tracker-bmyydhxe'\r\n",
      "    Running setup.py (path:/tmp/pip-req-build-o15qdqbv/setup.py) egg_info for package from file:///kaggle/input/nvidia-apex/apex-880ab92\r\n",
      "    Running command python setup.py egg_info\r\n",
      "    torch.__version__  =  1.2.0\r\n",
      "    running egg_info\r\n",
      "    creating pip-egg-info/apex.egg-info\r\n",
      "    writing pip-egg-info/apex.egg-info/PKG-INFO\r\n",
      "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\r\n",
      "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\r\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "    reading manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\r\n",
      "    /tmp/pip-req-build-o15qdqbv/setup.py:33: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\r\n",
      "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\r\n",
      "  Source in /tmp/pip-req-build-o15qdqbv has version 0.1, which satisfies requirement apex==0.1 from file:///kaggle/input/nvidia-apex/apex-880ab92\r\n",
      "  Removed apex==0.1 from file:///kaggle/input/nvidia-apex/apex-880ab92 from build tracker '/tmp/pip-req-tracker-bmyydhxe'\r\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\r\n",
      "Installing collected packages: apex\r\n",
      "  Created temporary directory: /tmp/pip-record-dbrkpjwq\r\n",
      "    Running command /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-o15qdqbv/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-o15qdqbv/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-dbrkpjwq/install-record.txt --single-version-externally-managed --compile\r\n",
      "    torch.__version__  =  1.2.0\r\n",
      "    /tmp/pip-req-build-o15qdqbv/setup.py:33: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\r\n",
      "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\r\n",
      "\r\n",
      "    Compiling cuda extensions with\r\n",
      "    nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "    Copyright (c) 2005-2018 NVIDIA Corporation\r\n",
      "    Built on Sat_Aug_25_21:08:01_CDT_2018\r\n",
      "    Cuda compilation tools, release 10.0, V10.0.130\r\n",
      "    from /usr/local/cuda/bin\r\n",
      "\r\n",
      "    running install\r\n",
      "    running build\r\n",
      "    running build_py\r\n",
      "    creating build\r\n",
      "    creating build/lib.linux-x86_64-3.6\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex\r\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof\r\n",
      "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\r\n",
      "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\r\n",
      "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\r\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\r\n",
      "    running build_ext\r\n",
      "    building 'apex_C' extension\r\n",
      "    creating build/temp.linux-x86_64-3.6\r\n",
      "    creating build/temp.linux-x86_64-3.6/csrc\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/opt/conda/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'amp_C' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'fused_adam_cuda' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'syncbn' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    building 'fused_layer_norm_cuda' extension\r\n",
      "    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\r\n",
      "    /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\r\n",
      "    running install_lib\r\n",
      "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.6/site-packages/apex/reparameterization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.6/site-packages/apex/amp/lists\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /opt/conda/lib/python3.6/site-packages/apex/amp\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /opt/conda/lib/python3.6/site-packages/apex/parallel\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.6/site-packages/apex/RNN\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/normalization\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/optimizers\r\n",
      "    creating /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.6/site-packages/apex/fp16_utils\r\n",
      "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.6/site-packages\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\r\n",
      "    byte-compiling /opt/conda/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\r\n",
      "    running install_egg_info\r\n",
      "    running egg_info\r\n",
      "    creating apex.egg-info\r\n",
      "    writing apex.egg-info/PKG-INFO\r\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\r\n",
      "    writing top-level names to apex.egg-info/top_level.txt\r\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    reading manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\r\n",
      "    Copying apex.egg-info to /opt/conda/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\r\n",
      "    running install_scripts\r\n",
      "    writing list of installed files to '/tmp/pip-record-dbrkpjwq/install-record.txt'\r\n",
      "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Removing source in /tmp/pip-req-build-o15qdqbv\r\n",
      "Successfully installed apex-0.1\r\n",
      "Cleaning up...\r\n",
      "Removed build tracker '/tmp/pip-req-tracker-bmyydhxe'\r\n",
      "1 location(s) to search for versions of pip:\r\n",
      "* https://pypi.org/simple/pip/\r\n",
      "Getting page https://pypi.org/simple/pip/\r\n",
      "Found index url https://pypi.org/simple\r\n",
      "Getting credentials from keyring for https://pypi.org/simple\r\n",
      "Getting credentials from keyring for pypi.org\r\n",
      "Starting new HTTPS connection (1): pypi.org:443\r\n",
      "Could not fetch URL https://pypi.org/simple/pip/: connection error: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fc3260dab70>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)) - skipping\r\n",
      "Given no hashes to check 0 links for project 'pip': discarding no candidates\r\n"
     ]
    }
   ],
   "source": [
    "# Installing Nvidia Apex\n",
    "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /kaggle/input/nvidia-apex/apex-880ab92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os, sys, re, gc, pickle, operator, shutil, copy\n",
    "import time, datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, Sampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainedModel\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"./\"\n",
    "DATA_DIR = \"/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/\"\n",
    "BERT_MODEL_PATH = '/kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "output_model_file = \"bert_models.pt\"\n",
    "\n",
    "SEED = 2019\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 220\n",
    "epochs_for_sched = 1\n",
    "checkpoint_iter = 20000\n",
    "lr = 2.5e-5\n",
    "batch_size = 16\n",
    "warmup_proportion = 0.05\n",
    "grad_accumulation_steps = 2\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "aux_columns = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "label_column = 'target'\n",
    "pred_column = 'prediction'\n",
    "text_column = 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for randomness in pytorch\n",
    "def seed_torch(seed=SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# Convert target and identity columns to booleans\n",
    "def convert_dataframe_to_bool(df):\n",
    "    def convert_to_bool(df, col_name):\n",
    "        df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "\n",
    "    bool_df = df.copy()\n",
    "    for col in [label_column] + identity_columns + aux_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "def convert_lines(texts, max_seq_length, tokenizer):\n",
    "    max_seq_length -= 2\n",
    "    all_tokens = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if len(tokens) > max_seq_length:\n",
    "            tokens = tokens[-max_seq_length:]\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens+[\"[SEP]\"])\n",
    "        all_tokens.append(one_token)\n",
    "\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "\n",
    "class Toxic_comments(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_comments, targets=None, split=None, maxlen=256):\n",
    "        self.comments = tokenized_comments\n",
    "        self.targets = targets\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'valid', 'test'}\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.comments[index]\n",
    "        if self.targets is not None:\n",
    "            target = self.targets[index]\n",
    "            return comment, torch.FloatTensor(target)\n",
    "        else:\n",
    "            return comment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def get_lens(self):\n",
    "        lengths = np.fromiter(\n",
    "            ((min(self.maxlen, len(seq))) for seq in self.comments),\n",
    "            dtype=np.int32)\n",
    "        return lengths\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for sequence bucketing\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of comments, and targets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            comments, targets = zip(*batch)\n",
    "        else:\n",
    "            comments = batch\n",
    "\n",
    "        lengths = [len(c) for c in comments]\n",
    "        maxlen = max(lengths)\n",
    "        padded_comments = []\n",
    "        for i, c in enumerate(comments):\n",
    "            padded_comments.append(c+[0]*(maxlen - lengths[i]))\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            return torch.LongTensor(padded_comments), torch.stack(targets)\n",
    "        else:\n",
    "            return torch.LongTensor(padded_comments)\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_lens, bucket_size=None, batch_size=1024, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.shuffle = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_lens = sort_lens\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_lens)\n",
    "        self.weights = None\n",
    "\n",
    "        if not shuffle_data:\n",
    "            self.index = self.prepare_buckets()\n",
    "        else:\n",
    "            self.index = None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        assert weights >= 0\n",
    "        total = np.sum(weights)\n",
    "        if total != 1:\n",
    "            weights = weights / total\n",
    "        self.weights = weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_lens)\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = self.prepare_buckets(indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_lens)\n",
    "\n",
    "    def prepare_buckets(self, indices=None):\n",
    "        lengths = - self.sort_lens\n",
    "        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lengths)\n",
    "\n",
    "        if indices is None:\n",
    "            if self.shuffle:\n",
    "                indices = shuffle(np.arange(len(lengths), dtype=np.int32))\n",
    "                lengths = lengths[indices]\n",
    "            else:\n",
    "                indices = np.arange(len(lengths), dtype=np.int32)\n",
    "\n",
    "        #  bucket iterator\n",
    "        def divide_chunks(l, n):\n",
    "            if n == len(l):\n",
    "                yield np.arange(len(l), dtype=np.int32), l\n",
    "            else:\n",
    "                # looping till length l\n",
    "                for i in range(0, len(l), n):\n",
    "                    data = l[i:i + n]\n",
    "                    yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "\n",
    "        new_indices = []\n",
    "        extra_batch_idx = None\n",
    "        for chunk_index, chunk in divide_chunks(lengths, self.bucket_size):\n",
    "            # sort indices in bucket by descending order of length\n",
    "            indices_sorted = chunk_index[np.argsort(chunk)]\n",
    "\n",
    "            batch_idxes = []\n",
    "            for _, batch_idx in divide_chunks(indices_sorted, self.batch_size):\n",
    "                if len(batch_idx) == self.batch_size:\n",
    "                    batch_idxes.append(batch_idx.tolist())\n",
    "                else:\n",
    "                    assert extra_batch_idx is None\n",
    "                    assert batch_idx is not None\n",
    "                    extra_batch_idx = batch_idx.tolist()\n",
    "\n",
    "            # shuffling batches within buckets\n",
    "            if self.shuffle:\n",
    "                batch_idxes = shuffle(batch_idxes)\n",
    "            for batch_idx in batch_idxes:\n",
    "                new_indices.extend(batch_idx)\n",
    "\n",
    "        if extra_batch_idx is not None:\n",
    "            new_indices.extend(extra_batch_idx)\n",
    "\n",
    "        if not self.shuffle:\n",
    "            self.original_indices = np.argsort(indices_sorted).tolist()\n",
    "        return indices[new_indices]\n",
    "\n",
    "\n",
    "def prepare_loader(x, y=None, batch_size=1024, split=None):\n",
    "    assert split in {'train', 'valid', 'test'}\n",
    "    dataset = Toxic_comments(x, y, split, MAX_SEQUENCE_LENGTH)\n",
    "    if split == 'train':\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                bucket_size=batch_size*50, batch_size=batch_size)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                batch_size=batch_size, shuffle_data=False)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn), sampler.original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, pred_column):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[pred_column])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, pred_column):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[pred_column])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, pred_column):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[pred_column])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "def calculate_overall_auc(df, pred_col, label_col):\n",
    "    true_labels = df[label_col]\n",
    "    predicted_labels = df[pred_col]\n",
    "    return roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for the training process\n",
    "\n",
    "class NetSolver(object):\n",
    "\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, print_freq, filepath):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.print_freq = print_freq\n",
    "        self.filepath = filepath\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.criterion = self.criterion.to(device)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Set up some book-keeping variables for optimization.\n",
    "        \"\"\"\n",
    "        self.best_val_loss = 1e4\n",
    "        self.best_val_auc = 0.\n",
    "        self.loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.auc_history = []\n",
    "        self.val_auc_history = []\n",
    "        self.val_unbias_auc_history = []\n",
    "        self.val_preds = []\n",
    "        self.models = {}\n",
    "\n",
    "    def save_checkpoint(self, iteration):\n",
    "        \"\"\"Save model checkpoint.\n",
    "        \"\"\"\n",
    "        self.models[f'ckpt_{iteration}'] = self.model.state_dict()\n",
    "    \n",
    "    def save_final_state(self):\n",
    "        \"\"\"Save final states.\n",
    "        \"\"\"\n",
    "        state = {'model': self.models,\n",
    "                 'optimizer': self.optimizer.state_dict()\n",
    "                 }\n",
    "        torch.save(state, self.filepath)\n",
    "\n",
    "    def forward_pass(self, x, y):\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \"\"\"\n",
    "        x, y = x.to(device=device, dtype=torch.long), y.to(device=device, dtype=torch.float)\n",
    "        scores = self.model(x)\n",
    "        loss = self.criterion(scores, y)\n",
    "        return loss, torch.sigmoid(scores)\n",
    "\n",
    "    def train(self, loaders, iterations, val_df, val_original_indices, start_time):\n",
    "        \"\"\"Weight of network updated by apex, grad accumulation, model checkpoint.\n",
    "        \"\"\"\n",
    "        train_loader, val_loader = loaders\n",
    "        loader = iter(train_loader)\n",
    "        running_loss = 0.\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # start training for iterations\n",
    "        for i in range(iterations):\n",
    "            self.model.train()\n",
    "\n",
    "            try:\n",
    "                x, y = next(loader)\n",
    "            except:\n",
    "                loader = iter(train_loader)\n",
    "                x, y = next(loader)\n",
    "            loss, _ = self.forward_pass(x, y)\n",
    "            \n",
    "#             loss.backward()\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "            # gradient accumulation for larger batch size effect with smaller memory usage\n",
    "            if (i+1) % grad_accumulation_steps == 0:   # Wait for several backward steps\n",
    "                self.optimizer.step()                  # Now we can do an optimizer step\n",
    "                self.optimizer.zero_grad()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # verbose and checkpoint\n",
    "            if (i+1) % self.print_freq == 0 or (i+1) == iterations:\n",
    "                print(f'Iteration {i+1}:')\n",
    "                train_auc, _ = self.check_auc(train_loader, num_batches=50)\n",
    "                print('{\"metric\": \"Loss\", \"value\": %.4f}' % (running_loss/(i+1),))\n",
    "                print('{\"metric\": \"AUC\", \"value\": %.4f}' % (train_auc,))\n",
    "                \n",
    "                is_print = False if (i+1) < iterations else True\n",
    "                val_auc, val_unbias_auc, val_loss, val_scores = self.check_auc(\n",
    "                    val_loader, df=val_df, idxs=val_original_indices, is_print=is_print)\n",
    "                print('{\"metric\": \"Val. Loss\", \"value\": %.4f}' % (val_loss,))\n",
    "                print('{\"metric\": \"Val. AUC\", \"value\": %.4f}' % (val_auc,))\n",
    "                print('{\"metric\": \"Val. Unbiased AUC\", \"value\": %.4f}' % (val_unbias_auc,))\n",
    "                \n",
    "                self.loss_history.append(running_loss/(i+1))\n",
    "                self.val_loss_history.append(val_loss)\n",
    "                self.auc_history.append(train_auc)\n",
    "                self.val_auc_history.append(val_auc)\n",
    "                self.val_unbias_auc_history.append(val_unbias_auc)\n",
    "                self.val_preds.append(val_scores)\n",
    "                \n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('updating best val loss...')\n",
    "                    self.best_val_loss = val_loss\n",
    "                if val_unbias_auc > self.best_val_auc:\n",
    "                    print('updating best val auc...')\n",
    "                    self.best_val_auc = val_unbias_auc\n",
    "                    self.save_checkpoint(i+1)\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                print()\n",
    "                \n",
    "            if (time.time() - start_time) > 29000:\n",
    "                break\n",
    "        \n",
    "        self.save_final_state()\n",
    "\n",
    "    def check_auc(self, loader, num_batches=None, df=None, idxs=None, is_print=False):\n",
    "        \"\"\"Calculate metrics for validation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        targets, scores, losses = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for t, (x, y) in enumerate(loader):\n",
    "                l, score = self.forward_pass(x, y)\n",
    "                targets.append((y[:,0].cpu().numpy()>=0.5).astype(int))\n",
    "                scores.append(score[:,0].cpu().numpy())\n",
    "                losses.append(l.item())\n",
    "                if num_batches is not None and (t+1) == num_batches:\n",
    "                    break\n",
    "\n",
    "        targets = np.concatenate(targets)\n",
    "        scores = np.concatenate(scores)\n",
    "        auc = roc_auc_score(targets, scores)\n",
    "        loss = np.mean(losses)\n",
    "\n",
    "        if df is not None:\n",
    "            unbias_auc = check_unbias_auc(df, scores[idxs], is_print)\n",
    "            return auc, unbias_auc, loss, scores[idxs]\n",
    "\n",
    "        return auc, loss\n",
    "    \n",
    "\n",
    "def check_unbias_auc(df, scores, print_table=False):\n",
    "    \"\"\"Calculate metrics for validation\n",
    "    \"\"\"\n",
    "    df[pred_column] = scores\n",
    "    bias_metrics_df = compute_bias_metrics_for_model(df, identity_columns, pred_column, label_column)\n",
    "    unbias_auc = get_final_metric(bias_metrics_df, calculate_overall_auc(df, pred_column, label_column))\n",
    "    if print_table:\n",
    "        print(bias_metrics_df)\n",
    "    return unbias_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(object):\n",
    "    # one-cycle scheduler\n",
    "    SCHEDULES = set(['cosine', 'linear', 'linear_cosine'])\n",
    "\n",
    "    def __init__(self, optimizer, iterations, sched_profile='cosine', max_lr=3e-3,\n",
    "                 moms=(.95, .85), div_factor=25, warmup=0.3, final_div=None):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        assert sched_profile in self.SCHEDULES\n",
    "        self.sched_profile = sched_profile\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "            self.init_lrs = [lr/div_factor for lr in self.max_lrs]\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "            self.init_lrs = [max_lr/div_factor] * len(optimizer.param_groups)\n",
    "\n",
    "        self.final_div = final_div\n",
    "        if self.final_div is None: self.final_div = div_factor*1e4\n",
    "        self.final_lrs = [lr/self.final_div for lr in self.max_lrs]\n",
    "        self.moms = moms\n",
    "\n",
    "        self.total_iteration = iterations\n",
    "        self.up_iteration = int(self.total_iteration * warmup)\n",
    "        self.down_iteration = self.total_iteration - self.up_iteration\n",
    "\n",
    "        self.curr_iter = 0\n",
    "        self._assign_lr_mom(self.init_lrs, [moms[0]]*len(optimizer.param_groups))\n",
    "\n",
    "    def _assign_lr_mom(self, lrs, moms):\n",
    "        for param_group, lr, mom in zip(self.optimizer.param_groups, lrs, moms):\n",
    "            param_group['lr'] = lr\n",
    "            param_group['betas'] = (mom, 0.999)\n",
    "\n",
    "    def _annealing_cos(self, start, end, pct):\n",
    "        cos_out = np.cos(np.pi * pct) + 1\n",
    "        return end + (start-end)/2 * cos_out\n",
    "\n",
    "    def _annealing_linear(self, start, end, pct):\n",
    "        return start + pct * (end-start)\n",
    "    \n",
    "    def _annealing_function(self, curr_iter):\n",
    "        if self.sched_profile == 'cosine':\n",
    "            return self._annealing_cos\n",
    "        if self.sched_profile == 'linear':\n",
    "            return self._annealing_linear\n",
    "        if self.sched_profile == 'linear_cosine':\n",
    "            if curr_iter <= self.up_iteration:\n",
    "                return self._annealing_linear\n",
    "            else:\n",
    "                return self._annealing_cos\n",
    "    \n",
    "    def step(self):\n",
    "        self.curr_iter += 1\n",
    "        anneal = self._annealing_function(self.curr_iter)\n",
    "\n",
    "        if self.curr_iter <= self.up_iteration:\n",
    "            pct = self.curr_iter / self.up_iteration\n",
    "            curr_lrs = [anneal(min_lr, max_lr, pct) \\\n",
    "                            for min_lr, max_lr in zip(self.init_lrs, self.max_lrs)]\n",
    "            curr_moms = [anneal(self.moms[0], self.moms[1], pct) \\\n",
    "                            for _ in range(len(self.optimizer.param_groups))]\n",
    "        else:\n",
    "            pct = (self.curr_iter-self.up_iteration) / self.down_iteration\n",
    "            curr_lrs = [anneal(max_lr, final_lr, pct) \\\n",
    "                            for max_lr, final_lr in zip(self.max_lrs, self.final_lrs)]\n",
    "            curr_moms = [anneal(self.moms[1], self.moms[0], pct) \\\n",
    "                            for _ in range(len(self.optimizer.param_groups))]\n",
    "\n",
    "        self._assign_lr_mom(curr_lrs, curr_moms)\n",
    "\n",
    "def lr_range_test(train_loader, model, optimizer, criterion, start_lr=1e-7,\n",
    "                  end_lr=10, num_it=100, stop_div=True):\n",
    "    epochs = int(np.ceil(num_it/len(train_loader)))\n",
    "    n_groups = len(optimizer.param_groups)\n",
    "\n",
    "    if isinstance(start_lr, list) or isinstance(start_lr, tuple):\n",
    "        if len(start_lr) != n_groups:\n",
    "            raise ValueError(\"expected {} max_lr, got {}\".format(n_groups, len(start_lr)))\n",
    "        start_lrs = list(start_lr)\n",
    "    else:\n",
    "        start_lrs = [start_lr] * n_groups\n",
    "\n",
    "    if isinstance(end_lr, list) or isinstance(end_lr, tuple):\n",
    "        if len(end_lr) != n_groups:\n",
    "            raise ValueError(\"expected {} max_lr, got {}\".format(n_groups, len(end_lr)))\n",
    "        end_lrs = list(end_lr)\n",
    "    else:\n",
    "        end_lrs = [end_lr] * n_groups\n",
    "\n",
    "    curr_lrs = start_lrs*1\n",
    "    for param_group, lr in zip(optimizer.param_groups, curr_lrs):\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    n, lrs_logs, loss_log = 0, [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device=device, dtype=torch.long), y.to(device=device, dtype=torch.float)\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lrs_logs.append(curr_lrs)\n",
    "            loss_log.append(loss.item())\n",
    "\n",
    "            # update best loss\n",
    "            if n == 0:\n",
    "                best_loss, n_best = loss.item(), n\n",
    "            else:\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss, n_best = loss.item(), n\n",
    "\n",
    "            # update lr per iter with exponential schedule\n",
    "            n += 1\n",
    "            curr_lrs = [lr * (end_lr/lr) ** (n/num_it) for lr, end_lr in zip(start_lrs, end_lrs)]\n",
    "            for param_group, lr in zip(optimizer.param_groups, curr_lrs):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # stopping condition\n",
    "            if n == num_it or (stop_div and (loss.item() > 4*best_loss or torch.isnan(loss))):\n",
    "                break\n",
    "\n",
    "    print('minimum loss {}, at lr {}'.format(best_loss, lrs_logs[n_best]))\n",
    "    return lrs_logs, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ./pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./bert_config.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "class BertForCustomClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertForCustomClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(5)])\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                h = self.classifier(dropout(pooled_output))\n",
    "            else:\n",
    "                h += self.classifier(dropout(pooled_output))\n",
    "        return h / len(self.dropouts)\n",
    "\n",
    "# Loss\n",
    "class UnbiasLoss(nn.Module):\n",
    "    def __init__(self, main_loss_weight=3.0):\n",
    "        super(UnbiasLoss, self).__init__()\n",
    "        self.alpha = main_loss_weight\n",
    "    def forward(self, pred_scores, labels):\n",
    "        main_loss = nn.BCEWithLogitsLoss(weight=labels[:,-1])(pred_scores[:,0], labels[:,0])\n",
    "        aux_loss = nn.BCEWithLogitsLoss()(pred_scores[:,1:], labels[:,1:-1])\n",
    "        return self.alpha * main_loss + aux_loss\n",
    "\n",
    "# Build model and optimizer\n",
    "def model_optimizer_init(ft_lrs, num_labels=16):\n",
    "    print(\"Building model and optimizer...\")\n",
    "    pre_model = BertForSequenceClassification.from_pretrained('../working', num_labels=num_labels)\n",
    "    bert_config = BertConfig(WORK_DIR + 'bert_config.json')\n",
    "    model = BertForCustomClassification(bert_config, num_labels=num_labels)\n",
    "    model.bert = copy.deepcopy(pre_model.bert)\n",
    "\n",
    "    params_bert = list(model.bert.parameters())\n",
    "    params_cls = list(model.classifier.parameters())\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': params_bert, 'lr':ft_lrs[0]},\n",
    "        {'params': params_cls, 'lr':ft_lrs[1]}\n",
    "        ]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(optimizer_grouped_parameters)\n",
    "    \n",
    "    return model, optimizer\n",
    "    \n",
    "\n",
    "# Translate model from tensorflow to pytorch\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "    BERT_MODEL_PATH + 'bert_config.json',\n",
    "    WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "# Save config files in the same path as pretrained model\n",
    "# for model reloading (inference, resume training etc.)\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_x, train_y):\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    cv_indices = [(tr_idx, val_idx) for tr_idx, val_idx in kf.split(train_x, train_y)]\n",
    "    return cv_indices\n",
    "\n",
    "def load_and_preproc():\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    train_df[identity_columns] = train_df[identity_columns].copy().fillna(0)\n",
    "\n",
    "    sample_weights = np.ones(len(train_df))\n",
    "    sample_weights += train_df[identity_columns].values.sum(1) * 3\n",
    "    sample_weights += train_df[label_column].values * 8\n",
    "    sample_weights /= sample_weights.max()\n",
    "    train_tars = train_df[[label_column]+aux_columns+identity_columns].values\n",
    "    train_tars = np.hstack([train_tars, sample_weights[:,None]]).astype('float32')\n",
    "\n",
    "    train_df = convert_dataframe_to_bool(train_df)\n",
    "    df = train_df[[label_column]+identity_columns].copy()\n",
    "    df[label_column] = df[label_column].astype('uint8')\n",
    "\n",
    "    return train_df[text_column], train_tars, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_seq, train_tars, trn_df = load_and_preproc()\n",
    "# cv_indices = train_val_split(train_seq, (train_tars[:,0]>=0.5).astype(int))\n",
    "# trn_idx, val_idx = cv_indices[0]\n",
    "\n",
    "# print('tokenizing...')\n",
    "# t0 = time.time()\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=True)\n",
    "# train_seq = convert_lines(train_seq[trn_idx[:20000]], MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "# print('tokenizing complete in {:.0f} seconds.'.format(time.time()-t0))\n",
    "\n",
    "# x_train, x_val = train_seq[:16000], train_seq[16000:]\n",
    "# y_train, y_val = train_tars[trn_idx[:16000]], train_tars[trn_idx[16000:20000]]\n",
    "# train_loader = prepare_loader(x_train, y_train, batch_size, split='train')\n",
    "# val_loader, val_original_indices = prepare_loader(x_val, y_val, 32, split='valid')\n",
    "# val_df = trn_df.iloc[trn_idx[16000:20000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing...\n",
      "tokenizing complete in 3148 seconds.\n",
      "CPU times: user 52min 36s, sys: 9.05 s, total: 52min 45s\n",
      "Wall time: 52min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_seq, train_tars, trn_df = load_and_preproc()\n",
    "cv_indices = train_val_split(train_seq, (train_tars[:,0]>=0.5).astype(int))\n",
    "trn_idx, val_idx = cv_indices[0]\n",
    "\n",
    "print('tokenizing...')\n",
    "t0 = time.time()\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=True)\n",
    "train_seq = convert_lines(train_seq, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "print('tokenizing complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training preparation\n",
    "\n",
    "x_train, x_val = train_seq[trn_idx], train_seq[val_idx]\n",
    "y_train, y_val = train_tars[trn_idx], train_tars[val_idx]\n",
    "train_loader = prepare_loader(x_train, y_train, batch_size, split='train')\n",
    "val_loader, val_original_indices = prepare_loader(x_val, y_val, 32, split='valid')\n",
    "val_df = trn_df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_seq, train_tars, trn_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0M\n",
      "0.0M\n"
     ]
    }
   ],
   "source": [
    "print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "print(str(torch.cuda.memory_cached(device)/1000000 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_lrs = [lr, 2*lr]\n",
    "# model, optimizer = model_optimizer_init(ft_lrs)\n",
    "# lrs_logs, loss_log = lr_range_test(train_loader, model.to(device), optimizer, UnbiasLoss().to(device), \n",
    "#                                    start_lr=(5e-8,1e-7), end_lr=(5,10))\n",
    "# lrs_logs = list(zip(*lrs_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# ax.plot(lrs_logs[-1][10:-3], loss_log[10:-3])\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_xlabel('Learning rate')\n",
    "# ax.set_ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and optimizer...\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "seed_torch(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_train_steps = int(epochs_for_sched * len(train_loader) / grad_accumulation_steps)\n",
    "ft_lrs = [lr, 1.1*lr]\n",
    "\n",
    "model, optimizer = model_optimizer_init(ft_lrs)\n",
    "scheduler = OneCycleScheduler(optimizer, num_train_steps, sched_profile='linear', max_lr=ft_lrs,\n",
    "                              div_factor=40, warmup=warmup_proportion)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = UnbiasLoss().to(device)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "solver = NetSolver(model, criterion, optimizer, scheduler, checkpoint_iter, 'epk_1_'+output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439.112704M\n",
      "494.927872M\n"
     ]
    }
   ],
   "source": [
    "print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "print(str(torch.cuda.memory_cached(device)/1000000 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "Iteration 20000:\n",
      "{\"metric\": \"Loss\", \"value\": 0.1743}\n",
      "{\"metric\": \"AUC\", \"value\": 0.9641}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 0.1425}\n",
      "{\"metric\": \"Val. AUC\", \"value\": 0.9643}\n",
      "{\"metric\": \"Val. Unbiased AUC\", \"value\": 0.9300}\n",
      "updating best val loss...\n",
      "updating best val auc...\n",
      "\n",
      "Iteration 40000:\n",
      "{\"metric\": \"Loss\", \"value\": 0.1574}\n",
      "{\"metric\": \"AUC\", \"value\": 0.9727}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 0.1404}\n",
      "{\"metric\": \"Val. AUC\", \"value\": 0.9680}\n",
      "{\"metric\": \"Val. Unbiased AUC\", \"value\": 0.9332}\n",
      "updating best val loss...\n",
      "updating best val auc...\n",
      "\n",
      "Iteration 60000:\n",
      "{\"metric\": \"Loss\", \"value\": 0.1516}\n",
      "{\"metric\": \"AUC\", \"value\": 0.9805}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 0.1392}\n",
      "{\"metric\": \"Val. AUC\", \"value\": 0.9709}\n",
      "{\"metric\": \"Val. Unbiased AUC\", \"value\": 0.9393}\n",
      "updating best val loss...\n",
      "updating best val auc...\n",
      "\n",
      "Iteration 80000:\n",
      "{\"metric\": \"Loss\", \"value\": 0.1483}\n",
      "{\"metric\": \"AUC\", \"value\": 0.9876}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 0.1386}\n",
      "{\"metric\": \"Val. AUC\", \"value\": 0.9717}\n",
      "{\"metric\": \"Val. Unbiased AUC\", \"value\": 0.9400}\n",
      "updating best val loss...\n",
      "updating best val auc...\n",
      "\n",
      "Iteration 90244:\n",
      "{\"metric\": \"Loss\", \"value\": 0.1470}\n",
      "{\"metric\": \"AUC\", \"value\": 0.9892}\n",
      "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
      "2      homosexual_gay_or_lesbian           2202      0.856754  0.905616   \n",
      "7                          white           5089      0.862308  0.903294   \n",
      "6                          black           3043      0.865968  0.888425   \n",
      "5                         muslim           4240      0.889318  0.918854   \n",
      "4                         jewish           1577      0.909469  0.948054   \n",
      "0                           male           8969      0.936449  0.957370   \n",
      "1                         female          10817      0.938521  0.960334   \n",
      "3                      christian           8015      0.943746  0.969285   \n",
      "8  psychiatric_or_mental_illness           1010      0.946374  0.945810   \n",
      "\n",
      "   bnsp_auc  \n",
      "2  0.961091  \n",
      "7  0.964064  \n",
      "6  0.969074  \n",
      "5  0.964207  \n",
      "4  0.953949  \n",
      "0  0.960818  \n",
      "1  0.959625  \n",
      "3  0.951921  \n",
      "8  0.972397  \n",
      "{\"metric\": \"Val. Loss\", \"value\": 0.1379}\n",
      "{\"metric\": \"Val. AUC\", \"value\": 0.9721}\n",
      "{\"metric\": \"Val. Unbiased AUC\", \"value\": 0.9414}\n",
      "updating best val loss...\n",
      "updating best val auc...\n",
      "\n",
      "Training complete in 17886 seconds.\n"
     ]
    }
   ],
   "source": [
    "n_iter = num_train_steps * grad_accumulation_steps\n",
    "print('Start training.')\n",
    "t0 = time.time()\n",
    "solver.train((train_loader, val_loader), n_iter, val_df, val_original_indices, t0)\n",
    "print('Training complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013.807616M\n",
      "2246.049792M\n"
     ]
    }
   ],
   "source": [
    "print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "print(str(torch.cuda.memory_cached(device)/1000000 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
      "2      homosexual_gay_or_lesbian           2202      0.856329  0.895445   \n",
      "7                          white           5089      0.862554  0.907660   \n",
      "6                          black           3043      0.865930  0.894480   \n",
      "5                         muslim           4240      0.890181  0.920484   \n",
      "4                         jewish           1577      0.910818  0.948449   \n",
      "0                           male           8969      0.936326  0.956691   \n",
      "1                         female          10817      0.938560  0.960345   \n",
      "3                      christian           8015      0.943983  0.970264   \n",
      "8  psychiatric_or_mental_illness           1010      0.945645  0.943163   \n",
      "\n",
      "   bnsp_auc  \n",
      "2  0.964740  \n",
      "7  0.962437  \n",
      "6  0.967032  \n",
      "5  0.963635  \n",
      "4  0.954041  \n",
      "0  0.961253  \n",
      "1  0.959621  \n",
      "3  0.950671  \n",
      "8  0.973332  \n",
      "{\"metric\": \"Ckpt Val. Unbiased AUC\", \"value\": 0.9414}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "ckpt_weights = [2**e for e in range(len(solver.val_preds))]\n",
    "val_preds = np.average(solver.val_preds, weights=ckpt_weights, axis=0)\n",
    "val_unbias_auc = check_unbias_auc(val_df, val_preds, True)\n",
    "print('{\"metric\": \"Ckpt Val. Unbiased AUC\", \"value\": %.4f}' % (val_unbias_auc,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
